import streamlit as st
import requests

# ======================================================
# CONFIG
# ======================================================

API_URL = "http://localhost:5000/screen"

st.set_page_config(
    page_title="AI Resume Screener",
    layout="centered"
)

# ======================================================
# UI HEADER
# ======================================================

st.title("ü§ñ AI Resume Screener")
st.caption("FastAPI + Valkey + Ollama (Local LLM)")

st.markdown(
    """
Upload a **resume PDF** and paste the **job description**.
The system evaluates the candidate using a local LLM with caching.
"""
)

# ======================================================
# INPUTS
# ======================================================

resume_file = st.file_uploader(
    "üìÑ Upload Resume (PDF only)",
    type=["pdf"]
)

job_description = st.text_area(
    "üßæ Job Description",
    height=260,
    placeholder="Paste the full job description here..."
)

# ======================================================
# ACTION
# ======================================================

if st.button("üîç Screen Resume"):
    if not resume_file or not job_description.strip():
        st.error("Please upload a resume and provide a job description.")
    else:
        with st.spinner("Analyzing resume..."):
            try:
                response = requests.post(
                    API_URL,
                    files={"resume": resume_file},
                    data={"job_description": job_description},
                    timeout=600
                )

                if response.status_code != 200:
                    st.error(f"API Error: {response.text}")
                else:
                    result = response.json()

                    # ======================================================
                    # RESULTS
                    # ======================================================

                    cache_status = result.get("cache", "UNKNOWN")
                    if cache_status == "HIT":
                        st.success("‚ö° Result served from cache")
                    else:
                        st.info("üß† Result generated by LLM")

                    st.metric(
                        label="Match Score",
                        value=f"{result['match_score']}%"
                    )

                    st.subheader("üë§ Candidate")
                    st.write(result.get("candidate_name", "Unknown"))

                    st.subheader("‚úÖ Key Strengths")
                    if result["key_strengths"]:
                        for s in result["key_strengths"]:
                            st.markdown(f"- **{s}**")
                    else:
                        st.write("No major strengths identified.")

                    st.subheader("‚ùå Missing Critical Skills")
                    if result["missing_critical_skills"]:
                        for m in result["missing_critical_skills"]:
                            st.markdown(f"- {m}")
                    else:
                        st.write("No critical skill gaps detected.")

                    st.subheader("üìå Recommendation")
                    recommendation = result["recommendation"]
                    if recommendation.lower() == "interview":
                        st.success("Interview Recommended")
                    else:
                        st.error("Not Recommended")

                    st.subheader("üß† Recruiter Reasoning")
                    st.write(result["reasoning"])

            except requests.exceptions.RequestException as e:
                st.error(f"Failed to connect to API: {e}")

# ======================================================
# FOOTER
# ======================================================

st.markdown("---")
st.caption("Built with ‚ù§Ô∏è using FastAPI, Valkey, Ollama & Streamlit")
